{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf6f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "\n",
    "random_reddit = pd.read_excel('random_reddit_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "512a3e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_rr = random_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ed3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a808731",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_rr.drop(['submissions'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37689c1d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "10447    None\n",
       "10448    None\n",
       "10449    None\n",
       "10450    None\n",
       "10451    None\n",
       "Name: tokenized, Length: 10452, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_rr['tokenized'] = anon_rr['tokenized'].apply(lambda x: ast.literal_eval(x))\n",
    "anon_rr['tokenized'].apply(lambda x: random.shuffle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c99c9d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "10447    None\n",
       "10448    None\n",
       "10449    None\n",
       "10450    None\n",
       "10451    None\n",
       "Name: lemmatized, Length: 10452, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_rr['lemmatized'] = anon_rr['lemmatized'].apply(lambda x: ast.literal_eval(x))\n",
    "anon_rr['lemmatized'].apply(lambda x: random.shuffle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfbf7d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[cancer, already, let, cure]</td>\n",
       "      <td>[cure, already, cancer, let]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[too, many, species, of, people, extinction]</td>\n",
       "      <td>[too, extinction, people, of, many, specie]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[survive]</td>\n",
       "      <td>[survive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a, sandbox, would, be, it]</td>\n",
       "      <td>[a, it, be, sandbox, would]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[get, bed, out, of]</td>\n",
       "      <td>[bed, get, out, of]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10447</th>\n",
       "      <td>[tide, roll]</td>\n",
       "      <td>[tide, roll]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10448</th>\n",
       "      <td>[do, do, what, got, ta, ta, got, they, they]</td>\n",
       "      <td>[got, do, what, they, got, ta, they, ta, do]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10449</th>\n",
       "      <td>[if, a, to, it, my, i, on, give, needs, me, mo...</td>\n",
       "      <td>[to, me, on, need, turned, a, give, and, be, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10450</th>\n",
       "      <td>[companies, think, i, than, i, the, infrastruc...</td>\n",
       "      <td>[the, company, be, think, blaming, rather, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10451</th>\n",
       "      <td>[but, are, get, consuming, consume, blackouts,...</td>\n",
       "      <td>[to, we, get, and, anyways, do, demand, but, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10452 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokenized  \\\n",
       "0                           [cancer, already, let, cure]   \n",
       "1           [too, many, species, of, people, extinction]   \n",
       "2                                              [survive]   \n",
       "3                            [a, sandbox, would, be, it]   \n",
       "4                                    [get, bed, out, of]   \n",
       "...                                                  ...   \n",
       "10447                                       [tide, roll]   \n",
       "10448       [do, do, what, got, ta, ta, got, they, they]   \n",
       "10449  [if, a, to, it, my, i, on, give, needs, me, mo...   \n",
       "10450  [companies, think, i, than, i, the, infrastruc...   \n",
       "10451  [but, are, get, consuming, consume, blackouts,...   \n",
       "\n",
       "                                              lemmatized  \n",
       "0                           [cure, already, cancer, let]  \n",
       "1            [too, extinction, people, of, many, specie]  \n",
       "2                                              [survive]  \n",
       "3                            [a, it, be, sandbox, would]  \n",
       "4                                    [bed, get, out, of]  \n",
       "...                                                  ...  \n",
       "10447                                       [tide, roll]  \n",
       "10448       [got, do, what, they, got, ta, they, ta, do]  \n",
       "10449  [to, me, on, need, turned, a, give, and, be, o...  \n",
       "10450  [the, company, be, think, blaming, rather, i, ...  \n",
       "10451  [to, we, get, and, anyways, do, demand, but, c...  \n",
       "\n",
       "[10452 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d20e94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anon_cr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m christian_reddit \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchristian_red_final.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m christian_reddit \u001b[38;5;241m=\u001b[39m \u001b[43manon_cr\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anon_cr' is not defined"
     ]
    }
   ],
   "source": [
    "christian_reddit = pd.read_excel('christian_red_final.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3d75e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_cr = christian_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cfe1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_cr.drop(['submissions'], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d56117f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "23556    None\n",
       "23557    None\n",
       "23558    None\n",
       "23559    None\n",
       "23560    None\n",
       "Name: lemmatized, Length: 23561, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_cr['lemmatized'] = anon_cr['lemmatized'].apply(lambda x: ast.literal_eval(x))\n",
    "anon_cr['lemmatized'].apply(lambda x: random.shuffle(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "904ab1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "23556    None\n",
       "23557    None\n",
       "23558    None\n",
       "23559    None\n",
       "23560    None\n",
       "Name: tokenized, Length: 23561, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_cr['tokenized'] = anon_cr['tokenized'].apply(lambda x: ast.literal_eval(x))\n",
    "anon_cr['tokenized'].apply(lambda x: random.shuffle(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d65be4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[of, think, men, and, hope, the, most, the, go...</td>\n",
       "      <td>[spirit, people, they, life, are, men, and, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[first, it, and, contentious, the, approved, b...</td>\n",
       "      <td>[theory, the, need, such, and, into, theism, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[argument, yet, evolution, throw, theory, non,...</td>\n",
       "      <td>[with, a, go, trinity, have, design, to, yet, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[could, gaps, than, own, is, to, creator, to, ...</td>\n",
       "      <td>[far, earth, therein, do, present, that, creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[of, a, a, theory, any, bang, publication, ama...</td>\n",
       "      <td>[amazing, when, thing, be, a, publication, nob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>[was, so, was, are, he, that, saying, asleep, ...</td>\n",
       "      <td>[he, when, so, saying, wa, he, or, the, are, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23557</th>\n",
       "      <td>[listening, that, even, use, if, death, to, wa...</td>\n",
       "      <td>[that, and, to, to, want, describe, is, taste,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23558</th>\n",
       "      <td>[want, speaker, if, that, you, own, of, the, h...</td>\n",
       "      <td>[if, it, those, insert, that, over, out, view,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23559</th>\n",
       "      <td>[physical, referred, the, to, death, speaker]</td>\n",
       "      <td>[the, referred, death, physical, to, speaker]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23560</th>\n",
       "      <td>[greek, you, show, translating, to, in, physic...</td>\n",
       "      <td>[physical, to, re, which, translating, show, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23561 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tokenized  \\\n",
       "0      [of, think, men, and, hope, the, most, the, go...   \n",
       "1      [first, it, and, contentious, the, approved, b...   \n",
       "2      [argument, yet, evolution, throw, theory, non,...   \n",
       "3      [could, gaps, than, own, is, to, creator, to, ...   \n",
       "4      [of, a, a, theory, any, bang, publication, ama...   \n",
       "...                                                  ...   \n",
       "23556  [was, so, was, are, he, that, saying, asleep, ...   \n",
       "23557  [listening, that, even, use, if, death, to, wa...   \n",
       "23558  [want, speaker, if, that, you, own, of, the, h...   \n",
       "23559      [physical, referred, the, to, death, speaker]   \n",
       "23560  [greek, you, show, translating, to, in, physic...   \n",
       "\n",
       "                                              lemmatized  \n",
       "0      [spirit, people, they, life, are, men, and, an...  \n",
       "1      [theory, the, need, such, and, into, theism, b...  \n",
       "2      [with, a, go, trinity, have, design, to, yet, ...  \n",
       "3      [far, earth, therein, do, present, that, creat...  \n",
       "4      [amazing, when, thing, be, a, publication, nob...  \n",
       "...                                                  ...  \n",
       "23556  [he, when, so, saying, wa, he, or, the, are, w...  \n",
       "23557  [that, and, to, to, want, describe, is, taste,...  \n",
       "23558  [if, it, those, insert, that, over, out, view,...  \n",
       "23559      [the, referred, death, physical, to, speaker]  \n",
       "23560  [physical, to, re, which, translating, show, m...  \n",
       "\n",
       "[23561 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anon_cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89058a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_cr.to_excel('anon_christian_reddit.xlsx', index=False)\n",
    "anon_rr.to_excel('anon_random_reddit.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
